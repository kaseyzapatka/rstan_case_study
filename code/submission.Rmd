---
title: "Analysis Submission"
author: "Kasey Zapatka"
output:
  bookdown::pdf_document2:
    toc: no
    fig_caption: yes
    latex_engine: xelatex
    number_sections: false
    keep_tex: true
geometry:
- top=1in
- bottom=1in
- right=1in
- left=1in
link-citations: yes
mainfont: Times New Roman
fontsize: 12pt
header-includes:
    - \usepackage{rotating}
    - \usepackage{setspace}
    - \newcommand{\beginsupplement}{\setcounter{table}{0}  \renewcommand{\thetable}{A\arabic{table}} \setcounter{figure}{0} \renewcommand{\thefigure}{A\arabic{figure}}}
    - \usepackage{lineno}
    - \usepackage{fancyhdr}
    - \usepackage{caption}
    - \usepackage{float}
    - \usepackage[section]{placeins}
   
---

```{r chunk-settings, echo = FALSE}

knitr::opts_chunk$set(
  fig.align = "center",        # center align figures
  warnings = FALSE,            # prevents warnings from appearing in code chunk
  message = FALSE,             # prevents messages from appearing in code chunk
  echo = FALSE,                 # do not echo code in file
  #results = "hide",            # hide results
  fig.keep = "all"             # keep all figs
)

```

```{r options}
# ==========================================================================
# PACKAGES & OPTIONS
# ==========================================================================
# clear all
remove(list = ls())

#
# Load libraries
# --------------------------------------------------------------------------
# Loading required packages and setting defaults
librarian::shelf(
  cmdstanr, tidyverse, tidybayes, posterior, bayesplot, timathomas/colorout,
  ggdark, patchwork, here, qs, knitr, gt
)


#
# Use cmdrstan for all R chunks
# --------------------------------------------------------------------------
register_knitr_engine(override = TRUE)


# data path
data_path <- "/Users/Dora/git/projects/rstan_case_study/data/" 

# read in diagnose output
diagnose_output <- qread(here("output/tables/testing.qs"))

```

## Step 1: Load data {-}

My code below uses a function to load `channel-spend.rds` into my R session and then calls that function and assigns the output to the `channels` variable.

```{r data, echo=TRUE}

# ==========================================================================
# STEP 1: LOAD DATA
# ==========================================================================

# STEP 1 -------------------------------------------
# function to load data
load_in_channel_data <- function(data_path){
  data <- read_rds(paste0(data_path, "channel-spend.rds"))
  glimpse(data)
}

# Call your function below
channels <- load_in_channel_data(data_path)
  
```


\newpage

## Step 2: Sample from the predict prior


### Do you think the priors we have set are reasonable? Yes, or No?

No, I do not think the set priors are reasonable. Summary Table \@ref(tab:summarytable) shows that the prior predictive covers a range (`7` and `28`) that is greater than what we might observe in our data (`4` to `9`). While I don't expect the priors to perfectly capture the observed data, this is a red flag that suggests the prior does not adequately capture the range of possible, observed values. 

```{r tables}

library(dplyr)
library(knitr)
library(kableExtra)

summary_table <- tribble(
  ~Statistic,       ~`Observed`, ~`Predicted`,
  "Min",            4.210,     7.281,
  "1st Quartile",   6.890,    13.070,
  "Median",         7.390,    16.326,
  "Mean",           7.266,    17.913,
  "3rd Quartile",   7.892,    23.068,
  "Max",            9.364,    28.278
)

summary_table %>%
  kbl(
    caption = "Summary statistics comparing observed and predicted values.",
    booktabs = TRUE,
    format = "latex",
    label = "summarytable"
  ) %>%
  kable_styling(latex_options = c("hold_position"))
```

### What is your justification for your answer above?

Panel A in Figure \@ref(fig:step2) visualizes the observed distribution with two peaks around `4.9` and `7.45`. The prior predictive in Panel B stands in stark contrast: there's a large bimodal distribution with peaks around (`~13` and `~23`), that are shifted to the right `~7` of the observed data, and of similar height. Although the prior captures the bimodal shape of the observed data, it is shifted too far to the right to capture the observed parameters and the left-hand peak is disproportionately large. Panel C, which plots daily revenue over time, suggests the observed bimodality may stem from a slower uptick in revenue at the start of the study period—perhaps due to unmodeled seasonality.

```{r step2, results = "markup", fig.cap = " ", out.height = "75%", out.width = "75%"}
 
knitr::include_graphics(here("output/figures/step2.png"))

```


\newpage

## Step 3: Sample from the posterior prior {-}

### What is the output of fit$cmdstan_diagnose()? What does this information tell you about the model you just sampled?

Figure @ref(fig:diagnose) shows several divergent transitions, indicating that the Hamiltonian Monte Carlo sampler failed to adequately explore the posterior. As a result, the model estimates are likely biased and unreliable. I should revise the priors to help the model better explore the full range of plausible parameter values.

```{r diagnose, results = "markup", fig.cap = "Diagnostic output", out.height = "75%", out.width = "75%"}
 
knitr::include_graphics(here("output/figures/posterior_diagnostics.png"))

```


### If this was a real model for a real client, would you feel comfortable proceeding with this model? Why or why not?

No, I would not feel comfortable proceeding with this model if it was for a real client because this diagnostic is telling me the range of possible estimates was not fully explored and I have biased estimates. I would prefer to give clients a model that I am confident fully explored the posterior. This confirms what I suspected from Figure \@ref(fig:step2) and Table \@ref(tab:summarytable)---that the priors would not cover the range of reasonable values and the posterior would not match well with observed data. Figure \@ref(fig:prioronposterior) visuallly confirms this by overlaying the prior predictive on the posterior, illustrating a large part of the posterior (red) is not covered by the prior (blue).

```{r prioronposterior, results = "markup", fig.cap = " ", out.height = "75%", out.width = "75%"}
 
knitr::include_graphics(here("output/figures/plot_prior_on_posterior.png"))

```

\newpage

## Step 4: Update priors {-}

### If you think this model is deficient in some manner, try editing the priors in order to improve the model. What did you change?

I think the model is deficient and edited the priors to improve the model. Table \ref{tab:tablechanges} lists the changes I made:


```{r tablechanges}

library(dplyr)
library(knitr)
library(kableExtra)

# Create simplified intercept prior table
intercept_tbl <- tibble::tribble(
  ~Parameter, ~`Original`, ~`Updated`,
  "intercept_lb", 10, 2,
  "intercept_ub", 20, 6,
  "intercept_eta_mean", 0, 0,
  "intercept_eta_scale", 100, 1
)

# Render using kable with caption and LaTeX formatting
intercept_tbl %>%
  kbl(
    caption = "Comparison of original and updated priors for the model intercept.",
    booktabs = TRUE,
    format = "latex",
    label = "tablechanges"
  ) %>%
  kable_styling(latex_options = c("hold_position"))

```

### Why did you change the priors you selected?

My main concern was addressing the baseline level of the outcome, which the original intercept prior failed to capture correctly. I narrowed the intercept bounds from $10$–$20$ to $4$–$10$, but prior predictive checks showed the mean was still too far to the right, so I adjusted them further to $2$–$6$, resulting in a mean closer to the empirical baseline of $7.5$. I also tightened `intercept_eta_scale` to $1$ to concentrate prior mass around the prior mean, and the updated prior and posterior predictive plots aligned well with the observed data. The priors for `beta`, `kappa`, `conc`, and `shift` were reasonable and didn't need changes after addressing the intercepts. Given the model's sensitivity to temporal structure, I chose not to alter the shift prior at this stage but may revisit it if future checks suggest the need.


### How do you know the model is better than before?

I know this model is better than before because (1) the prior includes reasonable estimates (`4.9` and `7.45`), (2) the updated posterior matches well to the observed, and (3) the output of `fit_updated$cmdstan_diagnose()` reports no divergent transitions. Panel B in Figure \@ref(fig:step4) illustrates point 1, Panel C illustrates point 2, and Panel A shows both points 1 and 2 cover the data well. 

\newpage

```{r step4, results = "markup", fig.cap = "  ", out.height = "90%", out.width = "90%"}
 
knitr::include_graphics(here("output/figures/step4.png"))

```
