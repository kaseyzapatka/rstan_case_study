---
title: "Analysis Submission"
author: "Kasey Zapatka"
date: " `r format(Sys.time(), '%B %d, %Y')`"
output:
  bookdown::pdf_document2:
    toc: no
    fig_caption: yes
    latex_engine: xelatex
    keep_tex: true
geometry:
- top=1in
- bottom=1in
- right=1in
- left=1in
link-citations: yes
mainfont: Times New Roman
fontsize: 12pt
header-includes:
    - \usepackage{rotating}
    - \usepackage{setspace}
    - \newcommand{\beginsupplement}{\setcounter{table}{0}  \renewcommand{\thetable}{A\arabic{table}} \setcounter{figure}{0} \renewcommand{\thefigure}{A\arabic{figure}}}
    - \usepackage{lineno}
    - \setlength\parindent{24pt}
    - \usepackage{fancyhdr}
    - \usepackage{caption}
   
---

```{r chunk-settings, echo = FALSE}

knitr::opts_chunk$set(
  fig.align = "center",        # center align figures
  warnings = FALSE,            # prevents warnings from appearing in code chunk
  message = FALSE,             # prevents messages from appearing in code chunk
  echo = FALSE,                 # do not echo code in file
  #results = "hide",            # hide results
  fig.keep = "all"             # keep all figs
)

```

```{r options}
# ==========================================================================
# PACKAGES & OPTIONS
# ==========================================================================
# clear all
remove(list = ls())

#
# Load libraries
# --------------------------------------------------------------------------
# Loading required packages and setting defaults
librarian::shelf(
  cmdstanr, tidyverse, tidybayes, posterior, bayesplot, timathomas/colorout,
  ggdark, patchwork, here
)


#
# Use cmdrstan for all R chunks
# --------------------------------------------------------------------------
register_knitr_engine(override = TRUE)


# data path
data_path <- "/Users/Dora/git/projects/rstan_case_study/data/" 

```

## Step 1: Load data {-}

<br> 

My code below uses a function to load `channel-spend.rds` into my R session and then calls that function and assigns the output to the `channels` variable.

```{r data, echo=TRUE}

# ==========================================================================
# STEP 1: LOAD DATA
# ==========================================================================

# STEP 1 -------------------------------------------
# function to load data
load_in_channel_data <- function(data_path){
  data <- read_rds(paste0(data_path, "channel-spend.rds"))
  glimpse(data)
}

# Call your function below
channels <- load_in_channel_data(data_path)
  
```


## Step 2: Sample from the predict prior  {-}


**Do you think the priors we have set are reasonable? Yes, or No?**

No, I do not think the set priors were reasonable. First, the prior predictive falls in a range that is greater than what we might observe in our data (falls between `10` and `25` instead of `4.5` to `9.0`). While I don't expect the priors to perfectly capture the observed data, it's a red flag to me that that the prior predictive falls so far above the observed data and that the summary statistics don't match (e.g. mean, median, standard deviation). Second, the priors predict a bimodal distribution with peaks of the same size. I would expect the first peak to be much small (perhaps it's just one peak with a left-skewed distribution?). 

**What is your justification for your answer above?**

<br> 

Figure \ref{fig:step} visualizes the observed distribution with two peaks around `4.9` and `7.45`. The prior predictive in Panel B stands in stark contrast: there's a large bimodal distribution with peaks much higher than in the observed data (`~13` and `~23`), shifted to the right `~7`, and of the same size. Although the prior captures the bimodal shape of the observed data, it is shifted too far to the right to capture the observed parameters and the left-hand side peak is too large. Exploratory analysis of the observed data in Panel C, which plots daily revenue over time, suggests that the observed bimodality stems from a slower uptick in revenue at the beginning of the study period — perhaps due to seasonality that still needs to be modeled.

```{r step, results = "markup", fig.cap = "", out.height = "50%", out.width = "75%"}
 
knitr::include_graphics(here("output/figures/step2.png"))

```


## Step 3: Sample from the posterior prior {-}

<br> 

**What is the output of fit$cmdstan_diagnose()? What does this information tell you about the model you just sampled?**

Figure \ref{fig:diagnose} shows that the main error was a number of divergent transitions, meaning the sampler (Hamiltonian Monte Carlo) was not able to sufficiently explore the posterior. This means that the model estimates are likely biased, that I should not trust them, and that I should adjust the priors to ensure the model can better check out all the possibilities for the parameters we care about. 


```{r diagnose, results = "markup", fig.cap = "", out.height = "50%", out.width = "75%"}
 
knitr::include_graphics(here("output/figures/posterior_diagnostics.png"))

```

<br> 


**If this was a real model for a real client, would you feel comfortable proceeding with this model? Why or why not?**

No, I would not feel comfortable proceeding with this model if it was for a real client because this diagnostic is telling me the range of possible estimates was not fully explored and I have biased estimates. I would prefer to give clients a model that I am confident fully explored the posterior. Visually, this makes sense since the posterior does not cover the range of the observed data (it's too far shifted to the right). Figure \ref{fig:prioronposterior} overlays the prior predictive on the posterior, illustrating the mismatch--a large part of the posterior (red) is not covered by the prior (blue).

```{r prioronposterior, results = "markup", fig.cap = "", out.height = "50%", out.width = "75%"}
 
knitr::include_graphics(here("output/figures/plot_prior_on_posterior.png"))

```

## Step 4: Update priors {-}

<br> 

**If you think this model is deficient in some manner, try editing the priors in order to improve the model. What did you change?**

I think the model is deficient and edited the priors to improve the model. Here is a table of changes I made:

<br>

```{r tableofchanges, results = "markup", fig.cap = "", out.height = "50%", out.width = "75%"}
 
knitr::include_graphics(here("output/tables/table_of_changes.png"))

```

**Why did you change the priors you selected?**

 - My main concern was to address the baseline level of the outcome that was not being correctly captured in the priors. I originally narrowed the range of the intercept (`intercept_lb` and `intercept_ub`) from $10$ and $20$ to $4$ and $10$ to match the range of the observed data and found it was still a bit far to the right. I narrowed the bounds further to $2$ and $6$ and found the prior predictive centered on a mean of about $7.5$, which matched observed data.  I altered `intercept_eta_scale` dramatically to 1 to ensure a tight fit of the intercept around the prior mean and was fairly confident in this prior given the prior and postierior predictive plots. 
 
 - The `beta`, `kappa`, and `conc` parameters seemed they didn't need to be altered given the updated posterior plot captured the observed well. I also  didn't want to alter the `shift` at this point, given this was a time series analysis and the model parameters seemed to capture the start of the upward trend after altering some of the other priors. 
 
 
 My main concern was to address the baseline level of the outcome, which was not being correctly captured by the original intercept prior. I originally narrowed the intercept bounds from $10$ and $20$ to $4$ and $10$, but prior predictive checks showed the mean was still too far to the right. I further narrowed it to $2$ and $6$ which resulted in a prior predictive mean around $7.5$ — better matching the empirical baseline. I also tightened intercept_eta_scale to $1$ to concentrate prior mass around this region. The updated prior and posterior predictive plots showed strong alignment with the observed data.
 
The priors for `beta`, `kappa`, and `conc` and `shift` appeared reasonable and did not require alteration. Posterior predictive checks showed these parameters contributed well to capturing the observed trend and variation. Given the model's sensitivity to temporal structure, I opted not to alter the shift prior at this stage, especially since the updated priors were sufficient to model the onset of the upward trend. I may revisit these priors if subsequent predictive checks or model performance suggest the need.

> How do you know the model is better than before?

I know this is better because the prior includes reasonable estimates (`4.9` and `7.45`). Figure \ref{fig:prioronposteriorupdated} illustrates this. 

```{r prioronposteriorupdated, results = "markup", fig.cap = "", out.height = "50%", out.width = "75%"}
 
knitr::include_graphics(here("output/figures/plot_prior_on_posterior_updated.png"))

```
