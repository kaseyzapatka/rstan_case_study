---
title: "Analysis Submission"
author: "Kasey Zapatka"
date: "`r Sys.Date()`"
output: html_document
---

```{r chunk-settings, echo = FALSE}

knitr::opts_chunk$set(
  fig.align = "center",        # center align figures
  warnings = FALSE,            # prevents warnings from appearing in code chunk
  message = FALSE,             # prevents messages from appearing in code chunk
  echo = FALSE,                 # do not echo code in file
  #results = "hide",            # hide results
  fig.keep = "all"             # keep all figs
)

```

```{r options}
# ==========================================================================
# PACKAGES & OPTIONS
# ==========================================================================
# clear all
remove(list = ls())

#
# Load libraries
# --------------------------------------------------------------------------
# Loading required packages and setting defaults
librarian::shelf(
  cmdstanr, tidyverse, tidybayes, posterior, bayesplot, timathomas/colorout,
  ggdark, patchwork, here
)


#
# Use cmdrstan for all R chunks
# --------------------------------------------------------------------------
register_knitr_engine(override = TRUE)


# data path
data_path <- "/Users/Dora/git/projects/rstan_case_study/data/" 

```

## Step 1: Load data

<br> 

See my code below that uses a function to load `channel-spend.rds` into my R session and then calls that function and assigns the output to the `channels` variable.


```{r data, echo=TRUE}

# ==========================================================================
# STEP 1: LOAD DATA
# ==========================================================================

# STEP 1 -------------------------------------------
# function to load data
load_in_channel_data <- function(data_path){
  data <- read_rds(paste0(data_path, "channel-spend.rds"))
  return(data)
}

# Call your function below
channels <- load_in_channel_data(data_path)
  
  glimpse(channels)

  
```


## Step 2: Sample from the predict prior 

<br> 

> Do you think the priors we have set are reasonable? Yes, or No?

No, I do not think the set priors were reasonable. First, the prior predictive falls in a range that is greater than what we might observe in our data (falls between `10` and `25` instead of `4.5` to `9.0`). So, that's the first red-flag. Second, the priors predict a bimodal distribution, it predicts both peaks be of the same size. I would expect the first peak to be much small (perhaps it's just one peak with a left-skewed distribution?). 

<br> 

> What is your justification for your answer above?

<br> 

Panel A in Figure \ref{fig:step2} visualizes the observed distribution and illustrates a bimodal distribution with a peak around `4.9` and `7.45`. The prior predictive in Panel B in Figure \ref{fig:step2} stands in contrast: a large bimodal distribution with peaks much higher than in the observed data (`~13` and `~23`), that is, shifted to the right `~7`. Although the prior captures the bimodal shape of the observed data, it is shifted way too far to the right to capture the observed parameters and the left-hand side peak is too large. Panel C of Figure \ref{fig:step2} plots each data point over time (daily revenue) and suggests the bimodal peak stems from slower uptick in daily revenue at the beginninng of the study period (perhaps we could modle this seasonality with more data points?).

```{r step2, results = "markup", fig.cap = "", out.width = "75%"}
 
knitr::include_graphics(here("output/figures/step2.png"))

```


## Step 3: Sample from the posterior prior 

<br> 

> What is the output of fit$cmdstan_diagnose()? What does this information tell you about the model you just sampled? 

Figure \ref{fig:diagnose} shows that the main error was a number of divergent transitions, meaning the sampler (Hamiltonian Monte Carlo) was not able to sufficently explore the posterior. This measn the the estimates it provides are likely biased and I should not trust them. 


```{r diagnose, results = "markup", fig.cap = "Research design to avoid simultaneity bias.", out.width = "75%"}
 
knitr::include_graphics(here("output/figures/posterior_diagnostics.png"))

```

<br> 


> If this was a real model for a real client, would you feel comfortable proceeding with this model? Why or why not?

No, I would not feel comfortable proceeding with this model if it was for a real client because this diagnostic is telling the posterior was not sufficenlty explored and my estimates are likely biased. Visually, this makes sense since the posterior does not cover the range of the observed data (it's too far shifted to the right). Figure \ref{fig:prioronposterior} overlays the prior predictive on the posterior, illustrating the mismatch--a large part of the posterior (red) is not covered by the prior (blue).

```{r prioronposterior, results = "markup", fig.cap = "Research design to avoid simultaneity bias.", out.width = "75%"}
 
knitr::include_graphics(here("output/figures/plot_prior_on_posterior.png"))

```

## Step 4: Update priors

<br> 

> If you think this model is deficient in some manner, try editing the priors in order to improve the model. What did you change?

Here is a table of changes I made:

<br> 
```{r}

# libraries 
library(gt)
library(dplyr)

# Create data with a group column
priors_tbl <- data.frame(
  Group = rep(c("Intercept", "Beta", "Kappa", "Concentration", "Shift"), each = 4),
  Parameter = c(
    "intercept_lb", "intercept_ub", "intercept_eta_mean", "intercept_eta_scale",
    "beta_lb", "beta_ub", "beta_eta_mean", "beta_eta_scale",
    "kappa_lb", "kappa_ub", "kappa_eta_mean", "kappa_eta_scale",
    "conc_lb", "conc_ub", "conc_eta_mean", "conc_eta_scale",
    "shift_lb", "shift_ub", "shift_eta_mean", "shift_eta_scale"
  ),
  Original = c(
    10, 20, 0, 100,
    0, 5, -1, 1,
    3, 10, -1, 1,
    0.3, 1.5, 0, 1,
    0, 10, 0, 1
  ),
  Updated = c(
    2, 6, 0, 1,
    0, 4, 0, 2,
    1, 6, 0, 2,
    0.3, 2.5, 0, 1.2,
    0, 10, 0, 4.5
  ),
  Changed = c(
    "Yes", "Yes", "No", "Yes",
    "No", "Yes", "Yes", "Yes",
    "Yes", "Yes", "Yes", "Yes",
    "No", "Yes", "No", "Yes",
    "No", "No", "No", "Yes"
  ),
  Rationale = c(
    "Narrowed range to match expected scale of outcome",
    "See above",
    "",
    "Strongly reduced scale to give tighter prior",
    "",
    "Slightly narrowed range for realism",
    "Centered prior around 0 instead of negative slope",
    "Allowed for both weak and strong slopes",
    "Allowed for both tall and flat curves",
    "Tightened upper bound for realism",
    "Made prior symmetric",
    "Allowed more variation in curvature",
    "",
    "Allowed for sharper peaks",
    "",
    "Allowed slightly more variation",
    "",
    "",
    "",
    "Very wide to cover both 4.9 and 7.45 modes"
  )
)

# Create gt table
priors_tbl %>%
  gt(groupname_col = "Group") %>%
  tab_header(title = "Comparison of Priors: Original vs. Updated") %>%
  cols_label(
    Parameter = "Parameter",
    Original = "Original Priors",
    Updated = "Updated Priors",
    Changed = "Changed?",
    Rationale = "Rationale for Change"
  ) %>%
  data_color(
    columns = c(Changed),
    colors = scales::col_factor(
      palette = c("white", "#4682B450"),  # lightened steelblue
      domain = c("No", "Yes")
    )
  ) %>%
  tab_options(
    row_group.as_column = TRUE,
    table.font.size = "small",
    row_group.font.weight = "bold",
    table.align = "left",
    heading.title.font.size = 16
  )


```


> Why did you change the priors you selected?

<br> 



> How do you know the model is better than before?

I know this is better because the prior includes reasonable estimates (`4.9` and `7.45`). Figure \ref{fig:prioronposteriorupdated} illustrates this. 

```{r prioronposteriorupdated, results = "markup", fig.cap = "Research design to avoid simultaneity bias.", out.width = "75%"}
 
knitr::include_graphics(here("output/figures/plot_prior_on_posterior_updated.png"))

```
